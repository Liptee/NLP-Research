Первым настоящим прорывом в решении проблемы токенизации богатых языков был сделан исследователями из Эддинбургского университета. Они создали подслова в нейронном машинном переводе, используя алгоритм BPE - **Byte Pair Encoding**.

Изначально BPE был представлен как простой алгоритм сжатия данных без потерь. В феврале 1994 года Филипп Гейдж в [статье](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM) описал метод, который работает так: самые частотные пары символов заменяются на другой символ, который не встречается в данных, при этом объем используемой памяти снижается с двух байт до одного.

Для задач NLP алгоритм BPE был немного изменен: часто встречающиеся группы символов не заменяются на другой символ, а объединяются в токен и добавляются в словарь. Алгоритм токенизации на основе BPE позволяет моделям узнавать как можно больше слов при ограниченном объеме словаря и выглядит так:
1. Создаем словарь
2. Представляем слова из текста как списки букв
3. Считаем количество вхождений каждой пары букв.
4. Объединяем самые частотные в токен и добавляем в словарь
5. Повторяем шаг 3 до тех пор, пока не получим словарь заданного размера

Сегодня схемы токенизации подслов стали нормой в самых продвинутых моделях, включая очень популярное семейство контекстных моделей, таких как [BERT], [GPT], [RoBERTa]

**Источники:**
- [Токенизация – обсуждение первого шага в обработке текста (sysblok.ru)](https://sysblok.ru/nlp/7250/?ysclid=ljsocqnijb112232662)