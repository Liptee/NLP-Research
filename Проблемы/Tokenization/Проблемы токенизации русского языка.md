## Проблема 1: языки с богатой морфологией
Это языки с развитыми системами склонений и спряжений слов. При работе с текстами на этих языках сложность возникает при составлении словаря, когда нужно найти и объединить все словоформы одной и той же лексемы.

Русский язык относится к таким языкам, благодаря различным падежам, временам глаголов, рода у существительных и прочего.

## Проблема 2: языки с продуктивным сложением основ
В германских языках (в английском, немецком, шведском и т.д.) очень продуктивно образуются новые сложные слова. Значения таких слов выводятся из значения их элементов, их можно создавать бесконечно долго, и большинство из них зафиксировано в "бумажном" словаре.

При работе с этими языками сложность также возникает на этапе создания словаря. При составлении словаря модели ориентируются на частотность (например, сохраняем слово, если оно встретилось чаще пяти раз), поэтому не будут запоминать такое длинное и сложное слово.

## Проблема 3: определение границ слова
Современные лингвисты до сих пор не могут придумать универсальное определение понятию "слово" и в каждой конкретной ситуации объясняют его по-разному. Для нас, привыкших я языкам европейского типа, слово - это набор букв между пробелами и знаками препинания. По таким разделителям компьютер тоже может легко найти слово.

Но в английском языке многие сложные слова пишутся раздельно, а в японском, наоборот, между словами вообще нет пробелов. Поэтому универсальный токенизатор создать было нелегко.

**Источники:**
- [Токенизация – обсуждение первого шага в обработке текста (sysblok.ru)](https://sysblok.ru/nlp/7250/?ysclid=ljsocqnijb112232662)