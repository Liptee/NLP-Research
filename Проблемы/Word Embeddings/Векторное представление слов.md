**Векторное представление слов** (*Word Embedding*) - это метод представления слов в виде числовых векторов в ML и NLP. Основная идея векторного представления слов заключается в том, чтобы присвоить каждому слову вектор в многомерном пространстве таким образом, чтобы близкие по семантике слова имели близкие векторы. Таким образом, слова, которые часто встречаются или используются в похожих контекстах, имеют близкие векторные представления.

#### Преимущества векторного представления слов
1. **Семантическая информация**. Векторные представления слов захватывают семантические свойства слов и их отношения. Близкие векторы указывают на семантическую близость слов, что позволяет модели понимать смысл и контекст в тексте.
2. **Сокращение размерности**. Векторные представления слов позволяют сократить размерность пространства признаков. Вместо использования большого словаря слов, можно представить слова в более низкоразмерном пространстве, сохраняя при этом значимую семантическую информацию.
3. **Улучшение моделей NLP**. Векторные представления слов используются в различных моделях и алгоритмах NLP для улучшения производительности и результатов. Они могут быть использованы для классификации текста, машинного перевода, извлечения информации, генерации текста и других задач.
4. **Решение проблемы редких слов**. Векторные представления слов могут помочь в обработке редких слов или слов, которых нет в обучающих данных. Благодаря контекстуальным подходам, модели могут обобщать смысл редких слов, опираясь на схожие слова и контекст, что полезно при работе с ограниченными данными.

Используются различные методы для создания векторных представлений слов, включая **методы на основе счетчиков**:
- [Word2Vec]
- [GloVe]
**контекстуальные модели**:
- [ELMo]
- [BERT]

#### Какой подход является актуальным для русского языка?